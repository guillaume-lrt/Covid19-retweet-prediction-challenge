{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJBzwHaKu7lw",
    "outputId": "7719346f-7018-412c-96b3-e29659b871a0"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "# !pip3 install pandas\n",
    "# !pip3 install sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# !pip install verstack\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4Z6JQ4J1Jpa",
    "outputId": "683195d6-3163-4c31-d33c-a577b147a076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id      timestamp  retweet_count  user_verified  \\\n",
      "0            0  1588696955143              0          False   \n",
      "1            1  1588464948124              0          False   \n",
      "2            2  1588634673360              0          False   \n",
      "3            3  1588433158672              0          False   \n",
      "4            4  1588582751599              0          False   \n",
      "...        ...            ...            ...            ...   \n",
      "665772  665772  1588412684317              0          False   \n",
      "665773  665773  1588324521711              1          False   \n",
      "665774  665774  1588353174952              8          False   \n",
      "665775  665775  1588691378352              0          False   \n",
      "665776  665776  1588432578764              0          False   \n",
      "\n",
      "        user_statuses_count  user_followers_count  user_friends_count  \\\n",
      "0                     68460                  1101                1226   \n",
      "1                       309                    51                 202   \n",
      "2                      3241                  1675                2325   \n",
      "3                     32327                   667                 304   \n",
      "4                       581                    42                 127   \n",
      "...                     ...                   ...                 ...   \n",
      "665772                65355                  1984                1902   \n",
      "665773                 1807                  2029                 347   \n",
      "665774                  888                    85                 257   \n",
      "665775                  452                    38                  91   \n",
      "665776                  590                   184                 238   \n",
      "\n",
      "       user_mentions                         urls hashtags  \\\n",
      "0                NaN                          NaN      NaN   \n",
      "1                NaN                          NaN      NaN   \n",
      "2                NaN                          NaN      NaN   \n",
      "3                NaN                          NaN      NaN   \n",
      "4                NaN                          NaN      NaN   \n",
      "...              ...                          ...      ...   \n",
      "665772           NaN                          NaN      NaN   \n",
      "665773  StanfordEMED  twitter.com/i/web/status/1…  COVID19   \n",
      "665774           NaN  twitter.com/i/web/status/1…      NaN   \n",
      "665775           NaN                          NaN      NaN   \n",
      "665776           NaN                          NaN      NaN   \n",
      "\n",
      "                                                     text  \n",
      "0                                           Smh I give up  \n",
      "1       Most of us are Human Beings, but I think you m...  \n",
      "2       Old dirty tricks Trump, at it again...like we ...  \n",
      "3       Seriously..... I worked 86 hours my last check...  \n",
      "4       May ALMIGHTY ALLAH have mercy on us all. Only ...  \n",
      "...                                                   ...  \n",
      "665772                     18 months dawg? Come on man...  \n",
      "665773  Thank you to all of the nurses in our @Stanfor...  \n",
      "665774  'Post it' pearls for Palliative, End of Life a...  \n",
      "665775  His facial expressions are kind of looking for...  \n",
      "665776                              We really can't wait.  \n",
      "\n",
      "[665777 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKc3ksh-vYtq",
    "outputId": "81821153-f37c-4d10-be35-f578ac12596d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error: 264.0506609594819\n"
     ]
    }
   ],
   "source": [
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweet_count'], axis=1)\n",
    "X_test = X_test.drop(['retweet_count'], axis=1)\n",
    "\n",
    "# You can examine the available features using X_train.head()\n",
    "\n",
    "# We set up an Tfidf Vectorizer that will use the top 100 tokens from the tweets. We also remove stopwords.\n",
    "# To do that we have to fit our training dataset and then transform both the training and testing dataset. \n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train['text'])\n",
    "X_test = vectorizer.transform(X_test['text'])\n",
    "\n",
    "# Now we can train our model. Here we chose a Gradient Boosting Regressor and we set our loss function \n",
    "reg = GradientBoostingRegressor()\n",
    "# We fit our model using the training data\n",
    "reg.fit(X_train, y_train)\n",
    "# And then we predict the values for our testing set\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Prediction error:\", mean_absolute_error(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uNh6HwYvnBO"
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Once we finalized our features and model we can train it using the whole training set and then produce prediction for the evaluating dataset\n",
    "###################################\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"evaluation.csv\")\n",
    "# Transform our data into tfidf vectors\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "y_train = train_data['retweet_count']\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "# We fit our model using the training data\n",
    "reg = GradientBoostingRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "X_val = vectorizer.transform(eval_data['text'])\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred = reg.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "affHkvHYNklS"
   },
   "outputs": [],
   "source": [
    "# Dump the results into a file that follows the required Kaggle template\n",
    "with open(\"gbr_predictions.txt\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred):\n",
    "        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrDKDPLpOh4H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM9auv+cd5h7pPWr0zCfGoI",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
