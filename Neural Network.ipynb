{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for multiclass classification\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import L1Loss\n",
    "from torch.nn import functional\n",
    "from torch.nn import InstanceNorm1d\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('data/train.csv')\n",
    "# X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "# X_train = X_train.drop(['text','hashtags','urls','user_mentions','retweet_count'], axis=1) # only numerical values\n",
    "# X_test = X_test.drop(['text','hashtags','urls','user_mentions','retweet_count'], axis=1) # only numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self):\n",
    "        # load the csv file as a dataframe\n",
    "#         df = read_csv(path, header=None)\n",
    "        df = read_csv('data/train.csv')\n",
    "        # store the inputs and outputs\n",
    "#         self.X = df.values[:, :-1]\n",
    "#         self.y = df.values[:, -1]\n",
    "#         vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "#         self.X = vectorizer.fit_transform(df['text'])\n",
    "        self.X = df.drop(['text','hashtags','urls','user_mentions','retweet_count'], axis=1)\n",
    "        self.y = df['retweet_count']\n",
    "        print(self.X)\n",
    "        # ensure input data is floats\n",
    "#         self.X = self.X.astype('float32')\n",
    "#         self.y = self.y.astype('float32')\n",
    "        self.X = self.X.values.astype(np.float32)\n",
    "        self.y = self.y.values.astype(np.int)\n",
    "#         print(self.y.shape)\n",
    "#         print(self.y)\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "        print(\"y:\",self.y)\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 8)\n",
    "#         xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(8, 4)\n",
    "#         xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(4, 1)\n",
    "#         xavier_uniform_(self.hidden3.weight)\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X\n",
    " \n",
    "# prepare the dataset\n",
    "def prepare_data():\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset()\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=10000, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    " \n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "#     criterion = MSELoss()\n",
    "    criterion = L1Loss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(10):\n",
    "        # enumerate mini batches\n",
    "        print(\"Epoch: \", epoch)\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "#             print(inputs)\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "#             print(targets,yhat)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "#             print(loss,'\\n')\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    " \n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "#         print(yhat)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "#     mse = mean_squared_error(actuals, predictions)\n",
    "#     return mse\n",
    "#     loss = L1Loss()\n",
    "    print(actuals.shape,predictions.shape)\n",
    "    print(actuals[:40],predictions[:40])\n",
    "    mae = mean_absolute_error(actuals,predictions)\n",
    "    return mae\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# make a single prediction (expect class=1)\n",
    "# row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "# yhat = predict(row, model)\n",
    "# print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id      timestamp  user_verified  user_statuses_count  \\\n",
      "0            0  1588696955143          False                68460   \n",
      "1            1  1588464948124          False                  309   \n",
      "2            2  1588634673360          False                 3241   \n",
      "3            3  1588433158672          False                32327   \n",
      "4            4  1588582751599          False                  581   \n",
      "...        ...            ...            ...                  ...   \n",
      "665772  665772  1588412684317          False                65355   \n",
      "665773  665773  1588324521711          False                 1807   \n",
      "665774  665774  1588353174952          False                  888   \n",
      "665775  665775  1588691378352          False                  452   \n",
      "665776  665776  1588432578764          False                  590   \n",
      "\n",
      "        user_followers_count  user_friends_count  \n",
      "0                       1101                1226  \n",
      "1                         51                 202  \n",
      "2                       1675                2325  \n",
      "3                        667                 304  \n",
      "4                         42                 127  \n",
      "...                      ...                 ...  \n",
      "665772                  1984                1902  \n",
      "665773                  2029                 347  \n",
      "665774                    85                 257  \n",
      "665775                    38                  91  \n",
      "665776                   184                 238  \n",
      "\n",
      "[665777 rows x 6 columns]\n",
      "y: [[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [8]\n",
      " [0]\n",
      " [0]]\n",
      "446071 219706\n",
      "Epoch:  0\n",
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n",
      "Epoch:  7\n",
      "Epoch:  8\n",
      "Epoch:  9\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = prepare_data()\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "model = MLP(6)\n",
    "\n",
    "train_model(train_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219706, 1) (219706, 1)\n",
      "[[  0]\n",
      " [  1]\n",
      " [  0]\n",
      " [  6]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  7]\n",
      " [  0]\n",
      " [  0]\n",
      " [  3]\n",
      " [  1]\n",
      " [  0]\n",
      " [  0]\n",
      " [195]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 78]\n",
      " [ 15]\n",
      " [  0]\n",
      " [  0]\n",
      " [312]\n",
      " [553]\n",
      " [ 22]\n",
      " [  7]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]] [[-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]\n",
      " [-0.00366375]]\n",
      "MAE: 87.737\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "mae = evaluate_model(test_dl, model)\n",
    "print('MAE: %.3f' % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "339 167\n",
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "epoch:  10\n",
      "epoch:  11\n",
      "epoch:  12\n",
      "epoch:  13\n",
      "epoch:  14\n",
      "epoch:  15\n",
      "epoch:  16\n",
      "epoch:  17\n",
      "epoch:  18\n",
      "epoch:  19\n",
      "epoch:  20\n",
      "epoch:  21\n",
      "epoch:  22\n",
      "epoch:  23\n",
      "epoch:  24\n",
      "epoch:  25\n",
      "epoch:  26\n",
      "epoch:  27\n",
      "epoch:  28\n",
      "epoch:  29\n",
      "epoch:  30\n",
      "epoch:  31\n",
      "epoch:  32\n",
      "epoch:  33\n",
      "epoch:  34\n",
      "epoch:  35\n",
      "epoch:  36\n",
      "epoch:  37\n",
      "epoch:  38\n",
      "epoch:  39\n",
      "epoch:  40\n",
      "epoch:  41\n",
      "epoch:  42\n",
      "epoch:  43\n",
      "epoch:  44\n",
      "epoch:  45\n",
      "epoch:  46\n",
      "epoch:  47\n",
      "epoch:  48\n",
      "epoch:  49\n",
      "epoch:  50\n",
      "epoch:  51\n",
      "epoch:  52\n",
      "epoch:  53\n",
      "epoch:  54\n",
      "epoch:  55\n",
      "epoch:  56\n",
      "epoch:  57\n",
      "epoch:  58\n",
      "epoch:  59\n",
      "epoch:  60\n",
      "epoch:  61\n",
      "epoch:  62\n",
      "epoch:  63\n",
      "epoch:  64\n",
      "epoch:  65\n",
      "epoch:  66\n",
      "epoch:  67\n",
      "epoch:  68\n",
      "epoch:  69\n",
      "epoch:  70\n",
      "epoch:  71\n",
      "epoch:  72\n",
      "epoch:  73\n",
      "epoch:  74\n",
      "epoch:  75\n",
      "epoch:  76\n",
      "epoch:  77\n",
      "epoch:  78\n",
      "epoch:  79\n",
      "epoch:  80\n",
      "epoch:  81\n",
      "epoch:  82\n",
      "epoch:  83\n",
      "epoch:  84\n",
      "epoch:  85\n",
      "epoch:  86\n",
      "epoch:  87\n",
      "epoch:  88\n",
      "epoch:  89\n",
      "epoch:  90\n",
      "epoch:  91\n",
      "epoch:  92\n",
      "epoch:  93\n",
      "epoch:  94\n",
      "epoch:  95\n",
      "epoch:  96\n",
      "epoch:  97\n",
      "epoch:  98\n",
      "epoch:  99\n",
      "MSE: 87.737, RMSE: 9.367\n",
      "Predicted: 21.288\n"
     ]
    }
   ],
   "source": [
    "### ONLINE TEST\n",
    "\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1].astype('float32')\n",
    "        self.y = df.values[:, -1].astype('float32')\n",
    "        # ensure target has the right shape\n",
    "        print(self.X)\n",
    "        print(self.y)\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        print(\"epoch: \",epoch)\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "#             print(targets,yhat)\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# define the network\n",
    "model = MLP(13)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "# make a single prediction (expect class=1)\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
