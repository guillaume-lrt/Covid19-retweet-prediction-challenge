{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'timestamp': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'user_verified': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'user_statuses_count': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, 'user_followers_count': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'user_friends_count': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'timestamp': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'user_verified': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'user_statuses_count': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, 'user_followers_count': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'user_friends_count': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "15558/15605 [============================>.] - ETA: 0s - loss: 38809136.0000 - mean_absolute_error: 38809136.0000WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'timestamp': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'user_verified': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'user_statuses_count': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, 'user_followers_count': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'user_friends_count': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "15605/15605 [==============================] - 13s 846us/step - loss: 38694420.0000 - mean_absolute_error: 38694420.0000 - val_loss: 146.3066 - val_mean_absolute_error: 146.3066\n",
      "Epoch 2/3\n",
      "15605/15605 [==============================] - 13s 830us/step - loss: 148.1594 - mean_absolute_error: 148.1594 - val_loss: 146.3058 - val_mean_absolute_error: 146.3058\n",
      "Epoch 3/3\n",
      "15605/15605 [==============================] - 15s 930us/step - loss: 148.1594 - mean_absolute_error: 148.1594 - val_loss: 146.3058 - val_mean_absolute_error: 146.3058\n",
      "15605/15605 [==============================] - 6s 409us/step - loss: 148.1593 - mean_absolute_error: 148.1593\n",
      "[148.1592559814453, 148.1592559814453]\n",
      "5202/5202 [==============================] - 2s 414us/step - loss: 146.3052 - mean_absolute_error: 146.3052\n",
      "Loss, Accuracy 146.30516052246094 146.30516052246094\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !pip3 install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "dataframe = pd.read_csv('data/train.csv')\n",
    "dataframe['user_verified'] = dataframe['user_verified'].astype(int)\n",
    "dataframe.head()\n",
    "\n",
    "dataframe = dataframe.drop(columns=['id', 'user_mentions', 'urls', 'hashtags', 'text'])\n",
    "\n",
    "train, val = train_test_split(dataframe, test_size=0.25)\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('retweet_count')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "                ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        return ds\n",
    "\n",
    "x_train = df_to_dataset(dataframe = train)\n",
    "x_eval = df_to_dataset(dataframe = val)\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count', 'user_friends_count']:\n",
    "        feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "model = tf.keras.Sequential([feature_layer, layers.Dense(64, activation='relu'), layers.Dense(128, activation='relu'),layers.Dense(64, activation='relu'),layers.Dense(32, activation='relu'),layers.Dense(16, activation='relu'),  layers.Dropout(.1), layers.Dense(1)])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError(), metrics=['mean_absolute_error'])\n",
    "\n",
    "model.fit(x_train, validation_data=x_eval, epochs=3, use_multiprocessing=True)\n",
    "\n",
    "print(model.evaluate(x_train))\n",
    "loss, accuracy = model.evaluate(x_eval)\n",
    "print(\"Loss, Accuracy\", loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
