{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(training=-1,testing=-1,all_dataset=False):\n",
    "    pickel_in = open(\"data/train_data_preprocessed.csv\", \"rb\")\n",
    "    train_data_prepro = pickle.load(pickel_in)\n",
    "    \n",
    "    pickel_in = open(\"data/evaluation_preprocessed.csv\", \"rb\")\n",
    "    eval_data_prepro = pickle.load(pickel_in)\n",
    "#     eval_data_prepro = pd.read_csv(\"data/evaluation.csv\",error_bad_lines=False)\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    sns.set(context=\"paper\")\n",
    "\n",
    "    if not all_dataset:\n",
    "#         X_train, X_test, y_train, y_test = scsplit(train_data_prepro, train_data_prepro['retweet_count'], stratify=train_data_prepro['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_data_prepro, train_data_prepro['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "    \n",
    "        if (training != -1):\n",
    "            if testing == -1:\n",
    "                testing = training\n",
    "            X_train = X_train.head(training)\n",
    "            X_test = X_test.head(testing)\n",
    "            y_train = y_train.head(training)\n",
    "            y_test = y_test.head(testing)\n",
    "            \n",
    "    else:\n",
    "        X_train = train_data_prepro\n",
    "        y_train = X_train['retweet_count']\n",
    "        X_test = -1\n",
    "        y_test = -1\n",
    "\n",
    "    # We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "    X_train = X_train.drop(['retweet_count'], axis=1)\n",
    "    \n",
    "    if not all_dataset:\n",
    "        X_test = X_test.drop(['retweet_count'], axis=1)\n",
    "\n",
    "    num_attribs = list(train_data_prepro[[\"user_verified\", \"timestamp_transf_hour\", \"timestamp_transf_weekday\", \"hashtags_count\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\"]])\n",
    "    text_attribs = \"text\"\n",
    "    bin_counting_nominal_cat_attribs = \"hashtags_transf\"\n",
    "\n",
    "\n",
    "    num_pipe = Pipeline([('std_scaler', StandardScaler())])\n",
    "    text_pipe = Pipeline([('tfidf_vect', TfidfVectorizer(max_features=25, stop_words='english'))])\n",
    "    bin_counting_nominal_cat_pipe = Pipeline([('count_vect', CountVectorizer(max_features=10))])\n",
    "\n",
    "    full_pipe = ColumnTransformer([\n",
    "        ('num', num_pipe, num_attribs),\n",
    "        ('text', text_pipe, text_attribs),\n",
    "        ('bin_counting', bin_counting_nominal_cat_pipe, bin_counting_nominal_cat_attribs),\n",
    "    ])\n",
    "\n",
    "    X_train = full_pipe.fit_transform(X_train)\n",
    "    if not all_dataset:\n",
    "        X_test = full_pipe.transform(X_test)\n",
    "    X_eval = full_pipe.transform(eval_data_prepro)\n",
    "\n",
    "    print(\"SHAPE OF X_train\", X_train.shape)\n",
    "    print(\"type(X_train) = \", type(X_train))\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"SHAPE OF y_train\", y_train.shape)\n",
    "    print(\"-----------------------------------\")\n",
    "    return X_train, X_test, y_train, y_test, X_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_train,y_train):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, print_features = False, all_dataset = False):  \n",
    "    pred_model_train = model.predict(X_train)\n",
    "    model_train_mae = mean_absolute_error(y_true=y_train, y_pred=pred_model_train)        \n",
    "    print(\"Logistic Regression prediction error for training set: \", model_train_mae) \n",
    "    if not all_dataset:\n",
    "        pred_model_test = model.predict(X_test)\n",
    "        model_test_mae = mean_absolute_error(y_true=y_test, y_pred=pred_model_test)\n",
    "        print(\"for testing set: \", model_test_mae)\n",
    "    \n",
    "    \n",
    "    if print_features:\n",
    "        #importances = log_reg.feature_importances_\n",
    "        importance = model.coef_[0]\n",
    "        for i,v in enumerate(importance):\n",
    "            print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "        # plot feature importance\n",
    "        plt.bar([x for x in range(len(importance))], importance)\n",
    "        plt.show()\n",
    "        \n",
    "    return model_train_mae, model_test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE OF X_train (466043, 42)\n",
      "type(X_train) =  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "-----------------------------------\n",
      "SHAPE OF y_train (466043,)\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_eval = get_data(training = -1, testing = -1, all_dataset=False)\n",
    "print(X_test == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Linear Regressor\\nprint(\"Linear Regressor\")\\n\\n# Lasso Regressor\\nprint(\"Lasso Regressor\")\\n\\n# Ridge Regressor\\nprint(\"Ridge Regressor\")\\n\\n# Elastic Net Regressor\\nprint(\"Elastic Net Regressor\")\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # # SELECT AND TRAIN MODELS # # # #\n",
    "train_mae_scores = []\n",
    "test_mae_scores = []\n",
    "\n",
    "'''\n",
    "# Linear Regressor\n",
    "print(\"Linear Regressor\")\n",
    "\n",
    "# Lasso Regressor\n",
    "print(\"Lasso Regressor\")\n",
    "\n",
    "# Ridge Regressor\n",
    "print(\"Ridge Regressor\")\n",
    "\n",
    "# Elastic Net Regressor\n",
    "print(\"Elastic Net Regressor\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linSVC = LinearSVC()\n",
    "train(linSVC,X_train,y_train)\n",
    "\n",
    "joblib.dump(linSVC, \"linSVC.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linSVC_train_mae, linSVC_test_mae = predict(linSVC, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 76.61278418302535 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rdf_reg.pkl']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_reg = RandomForestRegressor(bootstrap=False, max_depth=70, max_features='sqrt',\n",
    "                      min_samples_leaf=4, min_samples_split=10,\n",
    "                      n_estimators=2000, n_jobs=-1,scoring = \"neg_mean_absolute_error\")\n",
    "train(rdf_reg,X_train,y_train)\n",
    "\n",
    "joblib.dump(rdf_reg, \"rdf_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  180.9097614721928\n",
      "for testing set:  221.86609252172846\n"
     ]
    }
   ],
   "source": [
    "rdf_reg_train_mae, rdf_reg_test_mae = predict(rdf_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [1000, 1500, 2000], 'max_features': ['sqrt', 'log2'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 46.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 47.61357934077581 minutes ---\n",
      "--- Total time: 47.614506125450134 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "rf_random1 = RandomizedSearchCV(scoring = \"neg_mean_absolute_error\",estimator = rf, param_distributions = random_grid, verbose = 3, n_iter = 25, cv = 2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "train(rf_random,X_train, y_train)\n",
    "\n",
    "print(\"--- Total time: %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=False, max_depth=70, max_features='sqrt',\n",
      "                      min_samples_leaf=4, min_samples_split=10,\n",
      "                      n_estimators=2000, n_jobs=-1)\n",
      "Logistic Regression prediction error for training set:  175.7325555366603\n",
      "for testing set:  228.37667239954237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175.7325555366603, 228.37667239954237)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rf_random1.best_estimator_)\n",
    "predict(rf_random1.best_estimator_,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "score:  0.04065992886126546\n",
      "{'n_estimators': 2000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 70, 'bootstrap': False}\n",
      "30\n",
      "score:  0.04035516113341564\n",
      "{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 70, 'bootstrap': False}\n",
      "28\n",
      "score:  0.04014696487325975\n",
      "{'n_estimators': 2000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 40, 'bootstrap': True}\n",
      "42\n",
      "score:  0.03997506938652595\n",
      "{'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': True}\n",
      "23\n",
      "score:  0.03977353722220406\n",
      "{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
      "19\n",
      "score:  0.03974387163898546\n",
      "{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 80, 'bootstrap': True}\n",
      "45\n",
      "score:  0.03903964961895018\n",
      "{'n_estimators': 1200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
      "1\n",
      "score:  0.03901788479293866\n",
      "{'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'bootstrap': True}\n",
      "21\n",
      "score:  0.038816215284670774\n",
      "{'n_estimators': 1600, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 80, 'bootstrap': False}\n",
      "37\n",
      "score:  0.03871683162867223\n",
      "{'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': False}\n",
      "49\n",
      "score:  0.038493473636063535\n",
      "{'n_estimators': 800, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': False}\n",
      "20\n",
      "score:  0.03848456617633711\n",
      "{'n_estimators': 1600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 40, 'bootstrap': False}\n",
      "13\n",
      "score:  0.036692189877943204\n",
      "{'n_estimators': 1600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'bootstrap': False}\n",
      "46\n",
      "score:  0.03630211190468219\n",
      "{'n_estimators': 1200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 70, 'bootstrap': True}\n",
      "44\n",
      "score:  0.036233973293958444\n",
      "{'n_estimators': 1200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': True}\n",
      "36\n",
      "score:  0.03572643459349556\n",
      "{'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'bootstrap': True}\n",
      "18\n",
      "score:  0.03472842457517561\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 40, 'bootstrap': True}\n",
      "7\n",
      "score:  0.03456134854080212\n",
      "{'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 90, 'bootstrap': True}\n",
      "6\n",
      "score:  0.03437049072877946\n",
      "{'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 100, 'bootstrap': True}\n",
      "25\n",
      "score:  0.034340133710568155\n",
      "{'n_estimators': 1600, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40, 'bootstrap': True}\n",
      "17\n",
      "score:  0.03356340635403371\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 50, 'bootstrap': True}\n",
      "5\n",
      "score:  0.03323475411974197\n",
      "{'n_estimators': 1200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 80, 'bootstrap': True}\n",
      "8\n",
      "score:  0.032667954089605156\n",
      "{'n_estimators': 1600, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'bootstrap': True}\n",
      "47\n",
      "score:  0.0320003924303679\n",
      "{'n_estimators': 1200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 110, 'bootstrap': True}\n",
      "3\n",
      "score:  0.031587631938161\n",
      "{'n_estimators': 800, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': True}\n",
      "39\n",
      "score:  0.031509482467556915\n",
      "{'n_estimators': 2000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': True}\n",
      "34\n",
      "score:  0.03008478949003257\n",
      "{'n_estimators': 2000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10, 'bootstrap': True}\n",
      "35\n",
      "score:  0.027247101588011413\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': True}\n",
      "11\n",
      "score:  0.026863701519842798\n",
      "{'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'bootstrap': False}\n",
      "10\n",
      "score:  0.02617778562101547\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20, 'bootstrap': True}\n",
      "26\n",
      "score:  0.025003505539003934\n",
      "{'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10, 'bootstrap': False}\n",
      "29\n",
      "score:  -0.00479174705555252\n",
      "{'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 80, 'bootstrap': True}\n",
      "15\n",
      "score:  -0.0051096624443799366\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': True}\n",
      "14\n",
      "score:  -0.005939448890168175\n",
      "{'n_estimators': 2000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 30, 'bootstrap': True}\n",
      "33\n",
      "score:  -0.006941898304465899\n",
      "{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 10, 'bootstrap': True}\n",
      "43\n",
      "score:  -0.05449129682076159\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 30, 'bootstrap': True}\n",
      "12\n",
      "score:  -0.1490690533720107\n",
      "{'n_estimators': 1200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 110, 'bootstrap': True}\n",
      "27\n",
      "score:  -0.18291212242357613\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 90, 'bootstrap': True}\n",
      "41\n",
      "score:  -0.18572582611174426\n",
      "{'n_estimators': 2000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 30, 'bootstrap': True}\n",
      "38\n",
      "score:  -0.19162398129069702\n",
      "{'n_estimators': 2000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 20, 'bootstrap': True}\n",
      "32\n",
      "score:  -0.21357322683476376\n",
      "{'n_estimators': 1600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 90, 'bootstrap': False}\n",
      "24\n",
      "score:  -0.2495457654891563\n",
      "{'n_estimators': 800, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 20, 'bootstrap': False}\n",
      "22\n",
      "score:  -0.24959127842027296\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False}\n",
      "0\n",
      "score:  -0.24959936008633932\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 60, 'bootstrap': False}\n",
      "31\n",
      "score:  -0.39610120178647823\n",
      "{'n_estimators': 800, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': False}\n",
      "16\n",
      "score:  -0.39718359879416587\n",
      "{'n_estimators': 1200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 30, 'bootstrap': False}\n",
      "48\n",
      "score:  -0.47267836843514777\n",
      "{'n_estimators': 1200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 110, 'bootstrap': False}\n",
      "40\n",
      "score:  -0.473075405107087\n",
      "{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'bootstrap': False}\n",
      "4\n",
      "score:  -0.5157917539617741\n",
      "{'n_estimators': 800, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 100, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,50):\n",
    "    index = np.where(rf_random.cv_results_['rank_test_score'] == i)[0][0]\n",
    "    print(index)\n",
    "    print('score: ',rf_random.cv_results_['mean_test_score'][index])\n",
    "#     print(predict(rf_random.cv_results_['params'][i]))\n",
    "    print(rf_random.cv_results_['params'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(criterion='mse')\n",
    "train(tree_reg, X_train,y_train)\n",
    "\n",
    "joblib.dump(tree_reg, \"tree_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  0.1352853856001227\n",
      "for testing set:  262.85804610224653\n"
     ]
    }
   ],
   "source": [
    "tree_reg_train_mae, tree_reg_test_mae = predict(tree_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.8925456086794535 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gb_reg.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_reg = GradientBoostingRegressor(criterion='mse')\n",
    "train(gb_reg, X_train,y_train)\n",
    "\n",
    "joblib.dump(gb_reg, \"gb_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  220.47702797434127\n",
      "for testing set:  222.91790133523364\n"
     ]
    }
   ],
   "source": [
    "gb_reg_train_mae, gb_reg_test_mae = predict(gb_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # RANKING OF MODELS # # # #\n",
    "\n",
    "train_mae_scores.append(linSVC_train_mae)\n",
    "test_mae_scores.append(linSVC_test_mae)\n",
    "\n",
    "train_mae_scores.append(rdf_reg_train_mae)\n",
    "test_mae_scores.append(rdf_reg_test_mae)\n",
    "\n",
    "train_mae_scores.append(tree_reg_train_mae)\n",
    "test_mae_scores.append(tree_reg_test_mae)\n",
    "\n",
    "train_mae_scores.append(gb_reg_train_mae)\n",
    "test_mae_scores.append(gb_reg_test_mae)\n",
    "\n",
    "estimators = ['GradientBoostingRegressor',\n",
    "              'DecisionTreeRegressor',\n",
    "              'RandomForestRegressor',\n",
    "              'LinearSVC']\n",
    "\n",
    "bar_width = 0.10\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(len(estimators))\n",
    "training_scores1 = plt.barh(index, train_mae_scores, bar_width, color='darkred', alpha=0.6, label='Training Scores')\n",
    "test_scores1 = plt.barh(index+bar_width, test_mae_scores, bar_width, color='darkgreen', alpha=0.6, label='Test Scores')\n",
    "ax.set_title(\"Ranking of models by MAE scores\", fontsize=15, weight='bold')\n",
    "ax.set_xlabel('MAE')\n",
    "ax.set_ylabel('Estimators')\n",
    "ax.set_yticks(index+bar_width/2)\n",
    "ax.set_yticklabels(estimators)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
