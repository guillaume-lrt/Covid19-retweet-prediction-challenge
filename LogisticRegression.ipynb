{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(training=-1,testing=-1,all_dataset=False):\n",
    "    train_data = pd.read_csv(\"data/train.csv\")\n",
    "    eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "    pickel_in = open(\"data/train_data_preprocessed2.csv\", \"rb\")\n",
    "#     pickel_in = open(\"data/train_data_preprocessed2_augmented.csv\", \"rb\")\n",
    "    train_data_prepro = pickle.load(pickel_in)\n",
    "\n",
    "    train_data_prepro = pd.concat([train_data_prepro,\n",
    "                                     train_data[[\"timestamp\"]]],\n",
    "                                    axis=1)\n",
    "    \n",
    "    pickel_in = open(\"data/evaluation_preprocessed2.csv\", \"rb\")\n",
    "    eval_data_prepro = pickle.load(pickel_in)\n",
    "    \n",
    "    eval_data_prepro = pd.concat([eval_data_prepro,\n",
    "                                     eval_data[[\"timestamp\"]]],\n",
    "                                    axis=1)\n",
    "    \n",
    "    eval_data_prepro = eval_data_prepro.drop(['id','timestamp_transf_hour',\"timestamp_transf_weekday\"],axis=1)\n",
    "    train_data_prepro = train_data_prepro.drop(['timestamp_transf_hour',\"timestamp_transf_weekday\"],axis=1)\n",
    "    \n",
    "#     print(eval_data_prepro.head(10))\n",
    "#     print(train_data_prepro.head(10))\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    sns.set(context=\"paper\")\n",
    "\n",
    "    if not all_dataset:\n",
    "#         X_train, X_test, y_train, y_test = scsplit(train_data_prepro, train_data_prepro['retweet_count'], stratify=train_data_prepro['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_data_prepro, train_data_prepro['retweet_count'], train_size=0.80, test_size=0.20)\n",
    "    \n",
    "        if (training != -1):\n",
    "            if testing == -1:\n",
    "                testing = training\n",
    "            X_train = X_train.head(training)\n",
    "            X_test = X_test.head(testing)\n",
    "            y_train = y_train.head(training)\n",
    "            y_test = y_test.head(testing)\n",
    "            \n",
    "    else:\n",
    "        X_train = train_data_prepro\n",
    "        y_train = X_train['retweet_count']\n",
    "        X_test = -1\n",
    "        y_test = -1\n",
    "\n",
    "    # We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "    X_train = X_train.drop(['retweet_count'], axis=1)\n",
    "    \n",
    "    if not all_dataset:\n",
    "        X_test = X_test.drop(['retweet_count'], axis=1)\n",
    "\n",
    "    num_attribs = list(train_data_prepro[[\"user_verified\", \n",
    "                                          \"timestamp\",\n",
    "#                                           \"timestamp_transf_hour\", \n",
    "#                                           \"timestamp_transf_weekday\", \n",
    "                                          \"hashtags_count\",\n",
    "                                          \"user_statuses_count\", \n",
    "                                          \"user_followers_count\", \n",
    "                                          \"user_friends_count\",\n",
    "                                          \"user_mentions_transf\",\n",
    "                                          \"urls_transf\",\n",
    "                                          \"text_length\"]])\n",
    "    text_attribs = \"text\"\n",
    "    bin_counting_nominal_cat_attribs = \"hashtags_transf\"\n",
    "\n",
    "\n",
    "    num_pipe = Pipeline([('std_scaler', StandardScaler())])\n",
    "    text_pipe = Pipeline([('tfidf_vect', TfidfVectorizer(max_features=100, stop_words='english'))])\n",
    "    bin_counting_nominal_cat_pipe = Pipeline([('count_vect', CountVectorizer(max_features=50))])\n",
    "\n",
    "    full_pipe = ColumnTransformer([\n",
    "        ('num', num_pipe, num_attribs),\n",
    "        ('text', text_pipe, text_attribs),\n",
    "        ('bin_counting', bin_counting_nominal_cat_pipe, bin_counting_nominal_cat_attribs),\n",
    "    ])\n",
    "\n",
    "    X_train = full_pipe.fit_transform(X_train)\n",
    "    if not all_dataset:\n",
    "        X_test = full_pipe.transform(X_test)\n",
    "    X_eval = full_pipe.transform(eval_data_prepro)\n",
    "#     X_eval = -1\n",
    "    y_train = np.log(y_train+1)\n",
    "#     y_test = np.log(y_test+1)\n",
    "    \n",
    "    print(\"SHAPE OF X_train\", X_train.shape)\n",
    "    print(\"type(X_train) = \", type(X_train))\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"SHAPE OF y_train\", y_train.shape)\n",
    "    print(\"-----------------------------------\")\n",
    "    return X_train, X_test, y_train, y_test, X_eval\n",
    "\n",
    "\n",
    "def train(model,X_train,y_train):\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "    return model\n",
    "    \n",
    "def predict(model, print_features = False, all_dataset = False):  \n",
    "    pred_model_train = model.predict(X_train)\n",
    "    model_train_mae = mean_absolute_error(y_true=np.exp(y_train)-1, y_pred=np.exp(pred_model_train)-1)        \n",
    "    print(\"Logistic Regression prediction error for training set: \", model_train_mae) \n",
    "    if not all_dataset:\n",
    "        pred_model_test = np.exp(model.predict(X_test))-1\n",
    "        model_test_mae = mean_absolute_error(y_true=y_test, y_pred=pred_model_test)\n",
    "        print(\"for testing set: \", model_test_mae)\n",
    "    \n",
    "    \n",
    "    if print_features:\n",
    "        #importances = log_reg.feature_importances_\n",
    "        importance = log_reg.coef_[0]\n",
    "        for i,v in enumerate(importance):\n",
    "            print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "        # plot feature importance\n",
    "        plt.bar([x for x in range(len(importance))], importance)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE OF X_train (532621, 159)\n",
      "type(X_train) =  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "-----------------------------------\n",
      "SHAPE OF y_train (532621,)\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_eval = get_data(training = -1,all_dataset=False)\n",
    "print(X_test == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532621,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "--- 38.44895598491033 minutes ---\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(multi_class='ovr',n_jobs=-1)\n",
    "log_reg = train(log_reg,X_train,y_train)\n",
    "joblib.dump(log_reg, \"LogisticRegression.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-28e566259e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_reg' is not defined"
     ]
    }
   ],
   "source": [
    "predict(log_reg,print_features=True,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the results into a file that follows the required Kaggle template\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\",error_bad_lines=False)\n",
    "\n",
    "def write_file(model, title = \"LogisticRegression(42_features)_all_dataset\"):\n",
    "    y_pred = np.exp(model.predict(X_eval))-1\n",
    "    with open(title + \".txt\", 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "        for index, prediction in enumerate(y_pred):\n",
    "            writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [2,5,10,25]:\n",
    "    neigh = KNeighborsRegressor(n_neighbors=i,n_jobs=-1)\n",
    "    model = train(neigh,X_train,y_train)\n",
    "    title = \"neighbors_k\" + str(i) + \".pkl\"\n",
    "    joblib.dump(model, title)\n",
    "    predict(model,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Iteration 1, loss = 0.71525302\n",
      "Validation score: 0.612357\n",
      "Iteration 2, loss = 0.55981879\n",
      "Validation score: 0.630626\n",
      "Iteration 3, loss = 0.54151346\n",
      "Validation score: 0.636639\n",
      "Iteration 4, loss = 0.53317662\n",
      "Validation score: 0.632802\n",
      "Iteration 5, loss = 0.52697728\n",
      "Validation score: 0.640647\n",
      "Iteration 6, loss = 0.52202607\n",
      "Validation score: 0.645201\n",
      "Iteration 7, loss = 0.51773980\n",
      "Validation score: 0.646248\n",
      "Iteration 8, loss = 0.51442878\n",
      "Validation score: 0.642896\n",
      "Iteration 9, loss = 0.51252236\n",
      "Validation score: 0.645573\n",
      "Iteration 10, loss = 0.50891957\n",
      "Validation score: 0.646650\n",
      "Iteration 11, loss = 0.50694261\n",
      "Validation score: 0.643897\n",
      "Iteration 12, loss = 0.50430216\n",
      "Validation score: 0.646754\n",
      "Iteration 13, loss = 0.50269000\n",
      "Validation score: 0.646117\n",
      "Iteration 14, loss = 0.50085603\n",
      "Validation score: 0.645358\n",
      "Iteration 15, loss = 0.49844414\n",
      "Validation score: 0.645141\n",
      "Iteration 16, loss = 0.49680232\n",
      "Validation score: 0.648182\n",
      "Iteration 17, loss = 0.49545827\n",
      "Validation score: 0.646183\n",
      "Iteration 18, loss = 0.49355538\n",
      "Validation score: 0.648484\n",
      "Iteration 19, loss = 0.49257586\n",
      "Validation score: 0.646606\n",
      "Iteration 20, loss = 0.49048861\n",
      "Validation score: 0.645398\n",
      "Iteration 21, loss = 0.48915210\n",
      "Validation score: 0.643375\n",
      "Iteration 22, loss = 0.48836675\n",
      "Validation score: 0.647016\n",
      "Iteration 23, loss = 0.48652631\n",
      "Validation score: 0.646070\n",
      "Validation score did not improve more than tol=0.010000 for 20 consecutive epochs. Stopping.\n",
      "--- 3.19490255912145 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nn_regression.pkl']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(32,128,64,32,16,8,8),max_iter=200,tol = 0.01,n_iter_no_change = 20,learning_rate = 'adaptive',early_stopping=True,validation_fraction = 0.15,verbose=1)\n",
    "model = train(regr,X_train,y_train)\n",
    "joblib.dump(model, \"nn_regression.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  137.53091620194544\n",
      "for testing set:  155.64408649394656\n"
     ]
    }
   ],
   "source": [
    "predict(model,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Iteration 1, loss = 0.61329526\n",
      "Validation score: 0.621881\n",
      "Iteration 2, loss = 0.55294124\n",
      "Validation score: 0.632802\n",
      "Iteration 3, loss = 0.53498994\n",
      "Validation score: 0.633081\n",
      "Iteration 4, loss = 0.52287647\n",
      "Validation score: 0.638751\n",
      "Iteration 5, loss = 0.51343104\n",
      "Validation score: 0.639497\n",
      "Iteration 6, loss = 0.50531565\n",
      "Validation score: 0.642689\n",
      "Iteration 7, loss = 0.49833320\n",
      "Validation score: 0.643583\n",
      "Iteration 8, loss = 0.49151834\n",
      "Validation score: 0.642395\n",
      "Iteration 9, loss = 0.48413198\n",
      "Validation score: 0.642404\n",
      "Iteration 10, loss = 0.47781648\n",
      "Validation score: 0.640918\n",
      "Iteration 11, loss = 0.46990553\n",
      "Validation score: 0.637020\n",
      "Iteration 12, loss = 0.46228186\n",
      "Validation score: 0.636018\n",
      "Iteration 13, loss = 0.45469004\n",
      "Validation score: 0.636781\n",
      "Iteration 14, loss = 0.44812088\n",
      "Validation score: 0.626416\n",
      "Iteration 15, loss = 0.44044143\n",
      "Validation score: 0.626521\n",
      "Iteration 16, loss = 0.43334028\n",
      "Validation score: 0.626205\n",
      "Iteration 17, loss = 0.42701989\n",
      "Validation score: 0.626457\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "--- 2.537990379333496 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nn_regression2.pkl']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr2 = MLPRegressor(hidden_layer_sizes=(256,128,32),max_iter=200,learning_rate = 'adaptive',early_stopping=True,verbose=1,tol = 0.001,validation_fraction = 0.2,n_iter_no_change = 10)\n",
    "model2 = train(regr2,X_train,y_train)\n",
    "joblib.dump(model2, \"nn_regression2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  146.84982640875995\n",
      "for testing set:  168.53574028673216\n"
     ]
    }
   ],
   "source": [
    "predict(model2,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regr3 = MLPRegressor(hidden_layer_sizes=(64,128,128,64,32,16,8),max_iter=200,learning_rate = 'adaptive',early_stopping=True,verbose=1,tol = 0.00001,n_iter_no_change = 15)\n",
    "# model3 = train(regr3,X_train,y_train)\n",
    "# joblib.dump(model3, \"nn_regression3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  138.76219023105082\n",
      "for testing set:  146.46251520992584\n"
     ]
    }
   ],
   "source": [
    "predict(model3,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Iteration 1, loss = 0.61292405\n",
      "Validation score: 0.618130\n",
      "--- 0.12340400616327922 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nn_regression4.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr4 = MLPRegressor(hidden_layer_sizes=(64,128,64),max_iter=100,learning_rate = 'adaptive',validation_fraction = 0.2,early_stopping=True,verbose=1,tol = 0.00001,n_iter_no_change = 20)\n",
    "model4 = train(regr4,X_train,y_train)\n",
    "joblib.dump(model4, \"nn_regression4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  184.84210778608414\n",
      "for testing set:  182.41097344110497\n"
     ]
    }
   ],
   "source": [
    "predict(model4,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(model3,title=\"NeuralNetworkAugmented_All_Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [1000, 1500, 2000], 'max_features': ['sqrt', 'log2'], 'max_depth': [10, 24, 38, 52, 66, 80, 94, 108, 122, 136, 150, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 150, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# rf = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "# rf_random = RandomizedSearchCV(scoring = \"neg_mean_absolute_error\",estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, random_state=42, n_jobs = -1)\n",
    "# # Fit the random search model\n",
    "# train(rf_random,X_train, y_train)\n",
    "\n",
    "# print(\"--- Total time: %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-c61b539b6f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_random' is not defined"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_estimator_)\n",
    "predict(rf_random.best_estimator_,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "--- 3.605497244993846 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rdf_reg.pkl']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf_reg = RandomForestRegressor(max_depth=5, max_leaf_nodes = 16, criterion = 'mse',\n",
    "                      n_estimators=500, n_jobs=-1)\n",
    "train(rdf_reg,X_train,y_train)\n",
    "\n",
    "joblib.dump(rdf_reg, \"rdf_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  143.39218311079114\n",
      "for testing set:  150.94908086717808\n"
     ]
    }
   ],
   "source": [
    "predict(rdf_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_file(rdf_reg,title=\"random_forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "--- 8.291369120279947 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gb_reg.pkl']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_reg = GradientBoostingRegressor(criterion='mse', max_depth = 10, n_estimators = 100)\n",
    "train(gb_reg, X_train,y_train)\n",
    "\n",
    "joblib.dump(gb_reg, \"gb_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  126.75270591324156\n",
      "for testing set:  143.45199128104338\n"
     ]
    }
   ],
   "source": [
    "predict(gb_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(gb_reg,title=\"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'max_depth': [7,12], 'n_estimators': [85,115], 'learning_rate': [0.1]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total time: 12.837208513418833 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "gb_reg_search = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(gb_reg_search, param_grid, cv=2, return_train_score=True, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Total time: %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 115}\n",
      "GradientBoostingRegressor(max_depth=7, n_estimators=115)\n",
      "0.6733031155491349 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 85}\n",
      "0.6749588737038559 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 115}\n",
      "0.6720478763024451 {'learning_rate': 0.1, 'max_depth': 12, 'n_estimators': 85}\n",
      "0.6722412976369028 {'learning_rate': 0.1, 'max_depth': 12, 'n_estimators': 115}\n",
      "nan nan\n",
      "0.6731377907980847\n",
      "0.0011549122064959177\n",
      "nan\n",
      "nan\n",
      "Logistic Regression prediction error for training set:  133.2678600133127\n",
      "for testing set:  147.36643766382008\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "cvres = grid_search.cv_results_\n",
    "learn001 = []\n",
    "learn01 = []\n",
    "learn05 = []\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    if params['learning_rate'] == 0.01:\n",
    "        learn001.append(mean_score)\n",
    "    if params['learning_rate'] == 0.1:\n",
    "        learn01.append(mean_score)\n",
    "    if params['learning_rate'] == 0.5:\n",
    "        learn05.append(mean_score)\n",
    "    print(mean_score, params)\n",
    "\n",
    "\n",
    "print(np.mean(learn001), np.std(learn001))\n",
    "print(np.mean(learn01))\n",
    "print(np.std(learn01))\n",
    "print(np.mean(learn05))\n",
    "print(np.std(learn05))\n",
    "predict(grid_search.best_estimator_,print_features=False,all_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:2584.63306\n",
      "Will train until validation_0-rmse hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-rmse:2584.59595\n",
      "[2]\tvalidation_0-rmse:2584.56982\n",
      "[3]\tvalidation_0-rmse:2584.55029\n",
      "[4]\tvalidation_0-rmse:2584.53809\n",
      "[5]\tvalidation_0-rmse:2584.52783\n",
      "[6]\tvalidation_0-rmse:2584.52051\n",
      "[7]\tvalidation_0-rmse:2584.51440\n",
      "[8]\tvalidation_0-rmse:2584.50855\n",
      "[9]\tvalidation_0-rmse:2584.50537\n",
      "[10]\tvalidation_0-rmse:2584.50195\n",
      "[11]\tvalidation_0-rmse:2584.49902\n",
      "[12]\tvalidation_0-rmse:2584.49731\n",
      "[13]\tvalidation_0-rmse:2584.49609\n",
      "[14]\tvalidation_0-rmse:2584.49414\n",
      "[15]\tvalidation_0-rmse:2584.49365\n",
      "[16]\tvalidation_0-rmse:2584.49243\n",
      "[17]\tvalidation_0-rmse:2584.49170\n",
      "[18]\tvalidation_0-rmse:2584.49072\n",
      "[19]\tvalidation_0-rmse:2584.48901\n",
      "[20]\tvalidation_0-rmse:2584.48877\n",
      "[21]\tvalidation_0-rmse:2584.48828\n",
      "[22]\tvalidation_0-rmse:2584.48755\n",
      "[23]\tvalidation_0-rmse:2584.48755\n",
      "[24]\tvalidation_0-rmse:2584.48731\n",
      "[25]\tvalidation_0-rmse:2584.48706\n",
      "[26]\tvalidation_0-rmse:2584.48682\n",
      "[27]\tvalidation_0-rmse:2584.48633\n",
      "[28]\tvalidation_0-rmse:2584.48608\n",
      "[29]\tvalidation_0-rmse:2584.48608\n",
      "[30]\tvalidation_0-rmse:2584.48584\n",
      "[31]\tvalidation_0-rmse:2584.48511\n",
      "[32]\tvalidation_0-rmse:2584.48486\n",
      "[33]\tvalidation_0-rmse:2584.48511\n",
      "[34]\tvalidation_0-rmse:2584.48486\n",
      "[35]\tvalidation_0-rmse:2584.48486\n",
      "[36]\tvalidation_0-rmse:2584.48486\n",
      "[37]\tvalidation_0-rmse:2584.48462\n",
      "[38]\tvalidation_0-rmse:2584.48438\n",
      "[39]\tvalidation_0-rmse:2584.48438\n",
      "[40]\tvalidation_0-rmse:2584.48438\n",
      "[41]\tvalidation_0-rmse:2584.48413\n",
      "[42]\tvalidation_0-rmse:2584.48389\n",
      "[43]\tvalidation_0-rmse:2584.48389\n",
      "[44]\tvalidation_0-rmse:2584.48267\n",
      "[45]\tvalidation_0-rmse:2584.48267\n",
      "[46]\tvalidation_0-rmse:2584.48242\n",
      "[47]\tvalidation_0-rmse:2584.48242\n",
      "[48]\tvalidation_0-rmse:2584.48242\n",
      "[49]\tvalidation_0-rmse:2584.48242\n",
      "[50]\tvalidation_0-rmse:2584.48218\n",
      "[51]\tvalidation_0-rmse:2584.48218\n",
      "[52]\tvalidation_0-rmse:2584.48218\n",
      "[53]\tvalidation_0-rmse:2584.48218\n",
      "[54]\tvalidation_0-rmse:2584.48218\n",
      "[55]\tvalidation_0-rmse:2584.48193\n",
      "[56]\tvalidation_0-rmse:2584.48218\n",
      "[57]\tvalidation_0-rmse:2584.48169\n",
      "[58]\tvalidation_0-rmse:2584.48169\n",
      "[59]\tvalidation_0-rmse:2584.48120\n",
      "[60]\tvalidation_0-rmse:2584.48071\n",
      "[61]\tvalidation_0-rmse:2584.48071\n",
      "[62]\tvalidation_0-rmse:2584.48047\n",
      "[63]\tvalidation_0-rmse:2584.48047\n",
      "[64]\tvalidation_0-rmse:2584.48047\n",
      "[65]\tvalidation_0-rmse:2584.48022\n",
      "[66]\tvalidation_0-rmse:2584.48022\n",
      "[67]\tvalidation_0-rmse:2584.47998\n",
      "[68]\tvalidation_0-rmse:2584.48022\n",
      "[69]\tvalidation_0-rmse:2584.48022\n",
      "[70]\tvalidation_0-rmse:2584.48022\n",
      "[71]\tvalidation_0-rmse:2584.48022\n",
      "[72]\tvalidation_0-rmse:2584.48022\n",
      "Stopping. Best iteration:\n",
      "[67]\tvalidation_0-rmse:2584.47998\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=-1, nthread=-1, num_parallel_tree=1,\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             subsample=1, tree_method='exact', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = xgboost.XGBRegressor(nthread=-1)\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  135.12797181465436\n",
      "for testing set:  145.6330989741252\n"
     ]
    }
   ],
   "source": [
    "predict(xgb_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingRegressor(estimators=[('gb_reg',\n",
       "                             GradientBoostingRegressor(criterion='mse',\n",
       "                                                       max_depth=10)),\n",
       "                            ('xgb_reg',\n",
       "                             XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          gpu_id=-1, importance_type='gain',\n",
       "                                          interaction_constraints='',\n",
       "                                          learning_rate=0.300000012,\n",
       "                                          max_delta_step=0, max_depth=6,\n",
       "                                          min_child_weight=1...\n",
       "                                          reg_lambda=1, scale_pos_weight=1,\n",
       "                                          subsample=1, tree_method='exact',\n",
       "                                          validate_parameters=1,\n",
       "                                          verbosity=None)),\n",
       "                            ('nn2',\n",
       "                             MLPRegressor(early_stopping=True,\n",
       "                                          hidden_layer_sizes=(64, 128, 64, 32,\n",
       "                                                              16, 8),\n",
       "                                          learning_rate='adaptive',\n",
       "                                          n_iter_no_change=20,\n",
       "                                          validation_fraction=0.15,\n",
       "                                          verbose=1)),\n",
       "                            ('rdf',\n",
       "                             RandomForestRegressor(max_depth=5,\n",
       "                                                   max_leaf_nodes=16,\n",
       "                                                   n_estimators=500,\n",
       "                                                   n_jobs=-1))],\n",
       "                n_jobs=-1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# best_gb_regr = GradientBoostingRegressor(learning_rate=0.1,max_depth=10,n_estimators=100)\n",
    "\n",
    "voting_reg0 = VotingRegressor(\n",
    "    estimators=[('gb_reg', gb_reg), ('xgb_reg', xgb_reg),('nn2',model2),('rdf',rdf_reg)], n_jobs = -1\n",
    ")\n",
    "\n",
    "voting_reg0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  138.00320024996174\n",
      "for testing set:  146.75392498476893\n"
     ]
    }
   ],
   "source": [
    "predict(voting_reg0, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total time: 5.383676473299662 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "voting_MLP = MLPRegressor(hidden_layer_sizes=(64,32,16,8),max_iter=200,learning_rate = 'adaptive',early_stopping=True,verbose=1,tol = 0.000001,validation_fraction = 0.15,n_iter_no_change = 20)\n",
    "voting_XGBR = xgboost.XGBRegressor(nthread=-1)\n",
    "voting_GBR = GradientBoostingRegressor(criterion='mse', max_depth = 7, n_estimators = 100, n_iter_no_change=5,)\n",
    "rdf_reg = RandomForestRegressor(max_depth=50, max_leaf_nodes = 16, criterion = 'mse',\n",
    "                      n_estimators=1000, n_jobs=-1, min_samples_split = 4,max_features='sqrt')\n",
    "\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[('gb_reg', voting_GBR), ('xgb_reg', voting_XGBR),('nn2',voting_MLP),('rdf',rdf_reg)], n_jobs = -1\n",
    ")\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Total time: %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction error for training set:  139.58611859535534\n",
      "for testing set:  151.61104923322347\n"
     ]
    }
   ],
   "source": [
    "predict(voting_reg, print_features=False, all_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(voting_reg0,title=\"Voting Regression with MLP, XGBR, GBR and RDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
