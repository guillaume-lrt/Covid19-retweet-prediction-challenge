{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9auv+cd5h7pPWr0zCfGoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guillaume-lrt/Covid19-retweet-prediction-challenge/blob/main/Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJBzwHaKu7lw",
        "outputId": "7719346f-7018-412c-96b3-e29659b871a0"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "!pip install verstack\n",
        "from verstack.stratified_continuous_split import scsplit # pip install verstack"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting verstack\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/7e/6319afad955211755557db0f30c31a6eddd6cefcc795fec1c27bbb1b5e31/verstack-0.3.1.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from verstack) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from verstack) (1.18.5)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from verstack) (0.90)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from verstack) (0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->verstack) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->verstack) (2018.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost->verstack) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->verstack) (0.22.2.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->verstack) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->verstack) (0.17.0)\n",
            "Building wheels for collected packages: verstack\n",
            "  Building wheel for verstack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for verstack: filename=verstack-0.3.1-cp36-none-any.whl size=14343 sha256=6f85f282c2c668077ddb6a1443a7ae1165ae0af1f5661d2598230ed6cdbb00c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/1b/58/10e59516150cea4d9b1dbacceb3bffcc0cfd2d166efabec2f6\n",
            "Successfully built verstack\n",
            "Installing collected packages: verstack\n",
            "Successfully installed verstack-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Z6JQ4J1Jpa",
        "outputId": "683195d6-3163-4c31-d33c-a577b147a076"
      },
      "source": [
        "train_data = pd.read_csv('https://media.githubusercontent.com/media/guillaume-lrt/Covid19-retweet-prediction-challenge/main/data/train.csv',error_bad_lines=False)\n",
        "print(train_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ...                                               text\n",
            "0            0  ...                                      Smh I give up\n",
            "1            1  ...  Most of us are Human Beings, but I think you m...\n",
            "2            2  ...  Old dirty tricks Trump, at it again...like we ...\n",
            "3            3  ...  Seriously..... I worked 86 hours my last check...\n",
            "4            4  ...  May ALMIGHTY ALLAH have mercy on us all. Only ...\n",
            "...        ...  ...                                                ...\n",
            "665772  665772  ...                     18 months dawg? Come on man...\n",
            "665773  665773  ...  Thank you to all of the nurses in our @Stanfor...\n",
            "665774  665774  ...  'Post it' pearls for Palliative, End of Life a...\n",
            "665775  665775  ...  His facial expressions are kind of looking for...\n",
            "665776  665776  ...                              We really can't wait.\n",
            "\n",
            "[665777 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKc3ksh-vYtq",
        "outputId": "81821153-f37c-4d10-be35-f578ac12596d"
      },
      "source": [
        "# Load the training data\n",
        "# train_data = pd.read_csv(\"train.csv\")\n",
        "# train_data = pd.read_csv(\"https://github.com/guillaume-lrt/Covid19-retweet-prediction-challenge/blob/main/data/evaluation.csv\")\n",
        "\n",
        "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
        "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
        "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], train_size=0.7, test_size=0.3)\n",
        "\n",
        "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
        "X_train = X_train.drop(['retweet_count'], axis=1)\n",
        "X_test = X_test.drop(['retweet_count'], axis=1)\n",
        "\n",
        "# You can examine the available features using X_train.head()\n",
        "\n",
        "# We set up an Tfidf Vectorizer that will use the top 100 tokens from the tweets. We also remove stopwords.\n",
        "# To do that we have to fit our training dataset and then transform both the training and testing dataset. \n",
        "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(X_train['text'])\n",
        "X_test = vectorizer.transform(X_test['text'])\n",
        "\n",
        "# Now we can train our model. Here we chose a Gradient Boosting Regressor and we set our loss function \n",
        "reg = GradientBoostingRegressor()\n",
        "# We fit our model using the training data\n",
        "reg.fit(X_train, y_train)\n",
        "# And then we predict the values for our testing set\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"Prediction error:\", mean_absolute_error(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction error: 261.8316245537317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uNh6HwYvnBO"
      },
      "source": [
        "###################################\n",
        "# Once we finalized our features and model we can train it using the whole training set and then produce prediction for the evaluating dataset\n",
        "###################################\n",
        "# Load the evaluation data\n",
        "eval_data = pd.read_csv(\"https://media.githubusercontent.com/media/guillaume-lrt/Covid19-retweet-prediction-challenge/main/data/train.csv\",error_bad_lines=False)\n",
        "# Transform our data into tfidf vectors\n",
        "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "y_train = train_data['retweet_count']\n",
        "X_train = vectorizer.fit_transform(train_data['text'])\n",
        "# We fit our model using the training data\n",
        "reg = GradientBoostingRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "X_val = vectorizer.transform(eval_data['text'])\n",
        "# Predict the number of retweets for the evaluation dataset\n",
        "y_pred = reg.predict(X_val)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affHkvHYNklS"
      },
      "source": [
        "# Dump the results into a file that follows the required Kaggle template\n",
        "with open(\"gbr_predictions.txt\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
        "    for index, prediction in enumerate(y_pred):\n",
        "        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDKDPLpOh4H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}