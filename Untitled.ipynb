{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "######################\n",
    "# # # # OTHERS # # # #\n",
    "######################\n",
    "# todo: split training set into a training and test set. Then split the training set into a training and validation sets\n",
    "# todo: test all regressors and classifiers with regularization\n",
    "# todo: test NN with regularization\n",
    "# todo: select best models, create pipelines and combine them (ensemble learning)\n",
    "\n",
    "# X = np.linspace(0, 1, 100)\n",
    "# Y = np.linspace(0, 1, 100)\n",
    "# f1 = []\n",
    "# for i in range(100):\n",
    "#     for j in range(100):\n",
    "#         f1.append(2*(X[i]*Y[i])/(X[i]+Y[i]))\n",
    "#\n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes(projection='3d')\n",
    "# C = X + Y\n",
    "# ax.scatter(X, Y, f1, c=C)\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.show()\n",
    "\n",
    "# count_vec = CountVectorizer()\n",
    "# corpus = [\"This is the first document, document one\",\n",
    "#           \"This is the second document,\",\n",
    "#           \"And this is the third one.\",\n",
    "#           \"Is this the first document?\"]\n",
    "# X = count_vec.fit_transform(corpus)\n",
    "# print(\"X = \\n\", X)\n",
    "# print(\"X.type = \\n\", type(X))\n",
    "# print(\"X.shape = \\n\", X.shape)\n",
    "# print(\"X.toarray() = \\n\", X.toarray())\n",
    "# print(\"count_vec.get_feature_names() = \\n\", count_vec.get_feature_names())\n",
    "# print(\"corpus\", corpus)\n",
    "\n",
    "\n",
    "# # Tally occurrences of words in a list\n",
    "# cnt = Counter()\n",
    "# for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "#     cnt[word] += 1\n",
    "# print(\"cnt: \", cnt)  # Counter({'blue': 3, 'red': 2, 'green': 1})\n",
    "#\n",
    "# # pandas.DataFrame.replace\n",
    "# s = pd.Series([0, 1, 2, 3, 4])\n",
    "# df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n",
    "#                    'B': [5, 6, 3, 8, 9],\n",
    "#                    'C': ['a, b, c', 'b', 'c', 'd', 'e']})\n",
    "# print(\"s = \\n\", s)\n",
    "# s.replace(0,5, inplace=True)  # Attention: ajouter inplace=True ou s = s.replace(0,5) sinon Ã§a n'a aucun effet\n",
    "# print(\"s = \\n\", s)\n",
    "#\n",
    "# print(\"df = \\n\", df)\n",
    "# df.replace(0, 5, inplace=True)\n",
    "# print(\"df = \\n\", df)\n",
    "# df.replace([0, 1, 2, 3], 4, inplace=True)\n",
    "# print(\"df = \\n\", df)\n",
    "# df.replace([0, 1, 2, 3], [4, 3, 2, 1], inplace=True)\n",
    "# print(\"df = \\n\", df)\n",
    "# last = 'd'\n",
    "# print(last)\n",
    "# df.replace(last, 'COVID19_FV', inplace=True)\n",
    "# print(\"df = \\n\", df)\n",
    "#\n",
    "# df2 = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n",
    "#                     'B': ['abc', 'bar', 'xyz']})\n",
    "# print(\"df2 = \\n\", df2)\n",
    "# df2.replace(to_replace=r'^xy.$', value='new', regex=True, inplace=True)\n",
    "# print(\"df2 = \\n\", df2)\n",
    "\n",
    "# sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# # # # Load the training data # # # #\n",
    "######################################\n",
    "# train_data = pd.read_csv(\"../../iCloud Drive (archive)/Documents/Polytechnique_X/INF554/Project/train.csv\")\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "######################################\n",
    "# # # # AESTHETIC # # # #\n",
    "######################################\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sns.set(context=\"paper\")\n",
    "\n",
    "\n",
    "######################################\n",
    "# # # # QUICK LOOK AT THE DATA # # # #\n",
    "######################################\n",
    "print(\"Dataset shape:\", train_data.shape)\n",
    "print(\"Dataset head:\\n\", train_data.head(50))\n",
    "print(\"Dataset info:\\n\", train_data.info())\n",
    "print(\"Dataset desciption:\\n\", train_data.describe())\n",
    "\"\"\"\n",
    "# train_data.hist(bins=50)  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "# plt.suptitle(\"Distribution of each numerical attribute\", fontsize=15, weight='bold')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure()\n",
    "train_data['retweet_count'].hist(bins=50)  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distribution of retweet_count\", fontsize=15, weight='bold')\n",
    "plt.xlabel(\"retweet_count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Distribution of retweet_count per slices\n",
    "train_data_FV = train_data.copy()\n",
    "train_data_FV[\"retweet_count_cat\"] = pd.cut(train_data_FV[\"retweet_count\"],\n",
    "                                            bins=[0., 10., 20., 40., 60., 80.0, 110.0, 140., 180., 250., 300., 400., np.inf],\n",
    "                                            labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "train_data_FV[\"retweet_count_cat\"].hist()\n",
    "plt.title(\"Distribution of retweet_count per categories\", fontsize=15, weight='bold')\n",
    "plt.xlabel(\"retweet_count_cat\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Counts of unique values:\\ntrain_data_FV['retweet_count_cat'].value_counts() =\")\n",
    "print(train_data_FV['retweet_count_cat'].value_counts())\n",
    "\n",
    "\n",
    "# Pie Chart: Distribution of retweet_count per slices\n",
    "label_classes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "label_names = ['0-20', '20-40.', '40-60', '60-80', '80-110', '110-140', '140-180', '180-inf']\n",
    "sizes = [146234+22336, 18448, 18008, 8882, 5797, 5789, 4455, 4025+3683+3104+2213]\n",
    "colors = ['cadetblue', 'orange', 'yellowgreen', 'indianred', 'mediumpurple', 'darkblue', 'darkgreen', 'darkred']\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sizes, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax.legend(label_names, title=\"retweet_count in:\", loc=\"upper right\")\n",
    "ax.axis('equal')\n",
    "plt.suptitle('Quantity of instances per classes', fontsize=15, weight='bold')\n",
    "plt.title('Highlight of the imbalanced dataset', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "######################################\n",
    "# # # # DATA PREPROCESSING # # # #\n",
    "######################################\n",
    "train_data[\"user_mentions\"] = train_data[\"user_mentions\"].fillna('No user_mentions')\n",
    "train_data[\"urls\"] = train_data[\"urls\"].fillna('No urls')\n",
    "train_data[\"hashtags\"] = train_data[\"hashtags\"].fillna('No hashtag')\n",
    "\n",
    "# # # # Text Attributes # # # #\n",
    "\n",
    "# # user_mentions\n",
    "# print(\"user_mentions head:\\n\", train_data[\"user_mentions\"].head(10))\n",
    "# # print(\"user_mentions:\\n\", train_data[\"user_mentions\"].value_counts())\n",
    "# train_data[\"user_mentions\"] = train_data[\"user_mentions\"].fillna('No user_mentions')\n",
    "# print(\"user_mentions head:\\n\", train_data[\"user_mentions\"].head(10))\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder()\n",
    "# user_mentions_cat_encoded = one_hot_encoder.fit_transform(train_data[[\"user_mentions\"]])\n",
    "# print(\"user_mentions_cat_encoded =\", user_mentions_cat_encoded)\n",
    "#\n",
    "# # urls\n",
    "# print(\"urls head:\\n\", train_data[\"urls\"].head(10))\n",
    "# # print(\"urls:\\n\", train_data[\"urls\"].value_counts())\n",
    "# train_data[\"urls\"] = train_data[\"urls\"].fillna('No urls')\n",
    "# print(\"urls head:\\n\", train_data[\"urls\"].head(10))\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder()\n",
    "# urls_cat_encoded = one_hot_encoder.fit_transform(train_data[[\"urls\"]])\n",
    "# print(\"urls_cat_encoded =\", urls_cat_encoded)\n",
    "#\n",
    "# # hashtags\n",
    "# print(\"Hashtags head:\\n\", train_data[\"hashtags\"].head(10))\n",
    "# # print(\"Hashtags:\\n\", train_data[\"hashtags\"].value_counts())\n",
    "# train_data[\"hashtags\"] = train_data[\"hashtags\"].fillna('No hashtag')\n",
    "# print(\"Hashtags head:\\n\", train_data[\"hashtags\"].head(10))\n",
    "#\n",
    "# # from sklearn.preprocessing import OrdinalEncoder\n",
    "# # ordinal_encoder = OrdinalEncoder()\n",
    "# # hashtags_cat_encoded = ordinal_encoder.fit_transform(train_data[[\"hashtags\"]])\n",
    "# # print(hashtags_cat_encoded[:10])\n",
    "# # print(ordinal_encoder.categories_)\n",
    "#\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder()\n",
    "# hashtags_cat_encoded = one_hot_encoder.fit_transform(train_data[[\"hashtags\"]])\n",
    "# print(\"hashtags_cat_encoded =\", hashtags_cat_encoded)\n",
    "#\n",
    "# # text\n",
    "# tfidf_vect = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "# text_tfidf_vect = tfidf_vect.fit_transform(train_data[[\"text\"]])\n",
    "\n",
    "#X_train = vectorizer.fit_transform(X_train['text'])\n",
    "#X_test = vectorizer.transform(X_test['text'])\n",
    "\n",
    "\n",
    "# # # # DATA CLEANING # # # #\n",
    "train_data[\"user_mentions\"] = train_data[\"user_mentions\"].fillna('No user_mentions')\n",
    "train_data[\"urls\"] = train_data[\"urls\"].fillna('No urls')\n",
    "train_data[\"hashtags\"] = train_data[\"hashtags\"].fillna('No hashtag')\n",
    "\n",
    "\n",
    "###################################\n",
    "# # # # FEATURE ENGINEERING # # # #\n",
    "###################################\n",
    "\n",
    "# # # # FEATURE ENGINEERING: timestamp # # # #\n",
    "print(\"FEATURE ENGINEERING: timestamp --------------------\")\n",
    "pickel_in = open(\"train_data_timestamp_preprocessed.csv\", \"rb\")\n",
    "train_data_timestamp_preprocessed = pickle.load(pickel_in)\n",
    "\n",
    "\n",
    "# # # # FEATURE ENGINEERING: hashtags # # # #\n",
    "print(\"FEATURE ENGINEERING: hashtags --------------------\")\n",
    "pickel_in = open(\"train_data_hashtag_preprocessed.csv\", \"rb\")\n",
    "train_data_hashtag_preprocessed = pickle.load(pickel_in)\n",
    "\n",
    "\n",
    "\n",
    "# # # # FULL PREPROCESSED DATASET # # # #\n",
    "train_data = pd.concat([train_data_timestamp_preprocessed[[\"timestamp_transf_hour\", \"timestamp_transf_weekday\"]],\n",
    "                        train_data_hashtag_preprocessed[[\"hashtags_transf\", \"hashtags_count\"]],\n",
    "                        train_data[[\"retweet_count\", \"text\", \"user_verified\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\"]]],\n",
    "                       axis=1)\n",
    "\n",
    "print(\"train_data = \\n\", train_data.head(10))\n",
    "print(\"train_data rc =\", train_data[\"retweet_count\"].head())\n",
    "\n",
    "\n",
    "print(\"train_data = \\n\", train_data.head(10))\n",
    "\n",
    "train_data['hashtags_transf'] = train_data['hashtags_transf'].apply(lambda x: ','.join(map(str, x)))  # todo: put this line in data_pre_hashatags before pickling the dataset\n",
    "\n",
    "\n",
    "\n",
    "train_data = train_data[[\"timestamp_transf_hour\", \"timestamp_transf_weekday\", \"hashtags_count\", \"hashtags_transf\", \"user_verified\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\", \"text\", \"retweet_count\"]]\n",
    "dump(train_data, open('train_data_preprocessed.csv', 'wb'))\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "\"\"\"\n",
    "# # # # PREPROCESSING PIPELINE # # # #\n",
    "\n",
    "# Data Cleaning\n",
    "train_data[\"user_mentions\"] = train_data[\"user_mentions\"].fillna('No user_mentions')\n",
    "train_data[\"urls\"] = train_data[\"urls\"].fillna('No urls')\n",
    "train_data[\"hashtags\"] = train_data[\"hashtags\"].fillna('No hashtag')\n",
    "\n",
    "num_attributes = train_data.drop([\"user_mentions\", \"urls\", \"hashtags\", \"text\"], axis=1)\n",
    "cat_attributes = train_data[[\"user_mentions\", \"urls\", \"hashtags\"]].copy()\n",
    "text_attributes = train_data[\"text\"].copy()\n",
    "\n",
    "#num_attribs = [[\"timestamp\", \"user_verified\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\"]]\n",
    "num_attribs = list(num_attributes)\n",
    "cat_attribs = [\"user_mentions\", \"urls\", \"hashtags\"]\n",
    "text_attribs = \"text\"\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "#num_attributes_FV = num_pipe.fit_transform(num_attributes)\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "#cat_attributes_FV = cat_pipe.fit_transform(cat_attributes)\n",
    "\n",
    "text_pipe = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(max_features=100, stop_words='english'))\n",
    "])\n",
    "#text_attributes_FV = text_pipe.fit_transform(text_attributes)\n",
    "\n",
    "full_pipe = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_attribs),\n",
    "    (\"cat\", cat_pipe, cat_attribs),\n",
    "    (\"text\", text_pipe, text_attribs)\n",
    "])\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"SHAPE OF train_data\", train_data.shape)\n",
    "print(\"-----------------------------------\")\n",
    "X_TRAIN_FV = full_pipe.fit_transform(train_data)\n",
    "print(\"SHAPE OF X_TRAIN_FV\", X_TRAIN_FV.shape)\n",
    "print(\"type(X_TRAIN_FV) = \", type(X_TRAIN_FV))\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "X_TRAIN_FV = pd.DataFrame(X_TRAIN_FV.toarray())\n",
    "print(\"-----------------------------------\")\n",
    "print(\"SHAPE OF X_TRAIN_FV once pd.DF is applied\", X_TRAIN_FV.shape)\n",
    "print(\"type(X_TRAIN_FV) once pd.DF is applied\", type(X_TRAIN_FV))\n",
    "print(\"-----------------------------------\")\n",
    "X_TRAIN_FV[\"retweet_count\"] = list(train_data[\"retweet_count\"])\n",
    "print(\"SHAPE OF X_TRAIN_FV once pd.DF is applied and retweet_count added\", X_TRAIN_FV.shape)\n",
    "print(\"type(X_TRAIN_FV) once pd.DF is applied and retweet_count added\", type(X_TRAIN_FV))\n",
    "print(\"-----------------------------------\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # # # TARGET VALUES # # # #\n",
    "target_transformed = -1/(train_data['retweet_count']+1)+1\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle(\"Distribution of the preprocessed target\", fontsize=15, weight='bold')\n",
    "plt.subplot(2, 2, 1)\n",
    "train_data['retweet_count'].hist(bins=50)  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distrib of retweet_count\", fontsize=10, weight='bold')\n",
    "plt.xlabel(\"retweet_count\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(target_transformed, bins=50, label='Inverse Transformed retweet_count')  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distrib of -1/(1+retweet_count)+1\", fontsize=10, weight='bold')\n",
    "plt.xlabel(\"transf retweet_count\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(np.log1p(train_data['retweet_count']), bins=50, label='Natural Log-Transformed retweet_count')  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distrib of Log(1+retweet_count)\", fontsize=10, weight='bold')\n",
    "plt.xlabel(\"transf retweet_count\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(np.log1p(np.log1p(train_data['retweet_count'])), bins=50, label='Natural Log-Transformed retweet_count')  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distrib of Log(1+Log(1+retweet_count))\", fontsize=10, weight='bold')\n",
    "plt.xlabel(\"transf retweet_count\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "target_transformed = np.log1p(train_data['retweet_count'])\n",
    "target_transformed = (target_transformed - np.mean(target_transformed))/float(np.std(target_transformed))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(target_transformed, bins=50, label='Log')  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distribution of retweet_count\", fontsize=15, weight='bold')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "target_transformed = -1/(train_data['retweet_count']+1)+1\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(target_transformed, bins=50, label='Log')  # bins=round(np.sqrt(train_data.shape[0]))\n",
    "plt.title(\"Distribution of retweet_count\", fontsize=15, weight='bold')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# # # # TAKE A SMALLER SUBSET OF THE ENTIRE DATASET (to speed up) # # # #\n",
    "\n",
    "# train_data = train_data[[\"retweet_count\", \"user_verified\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\", \"text\", \"timestamp_transf_hour\", \"timestamp_transf_weekday\"]]\n",
    "# train_data = train_data[[\"retweet_count\", \"text\", \"timestamp_transf_hour\", \"timestamp_transf_weekday\", \"hashtags_transf\", \"hashtags_count\"]]\n",
    "\n",
    "# train_data = train_data.head(int(len(train_data)/10))\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "# # # # TRAINING SET - TEST SET # # # #\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "print(\"-----------------------------------\")\n",
    "print(\"TRAINING DONE\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweet_count'], axis=1)\n",
    "X_test = X_test.drop(['retweet_count'], axis=1)\n",
    "\n",
    "\n",
    "num_attribs = list(train_data[[\"user_verified\", \"timestamp_transf_hour\", \"timestamp_transf_weekday\", \"hashtags_count\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\"]])\n",
    "text_attribs = \"text\"\n",
    "bin_counting_nominal_cat_attribs = \"hashtags_transf\"\n",
    "\n",
    "\n",
    "num_pipe = Pipeline([('std_scaler', StandardScaler())])\n",
    "text_pipe = Pipeline([('tfidf_vect', TfidfVectorizer(max_features=100, stop_words='english'))])\n",
    "bin_counting_nominal_cat_pipe = Pipeline([('count_vect', CountVectorizer(max_features=20))])\n",
    "\n",
    "full_pipe = ColumnTransformer([\n",
    "    ('num', num_pipe, num_attribs),\n",
    "    ('text', text_pipe, text_attribs),\n",
    "    ('bin_counting', bin_counting_nominal_cat_pipe, bin_counting_nominal_cat_attribs),\n",
    "])\n",
    "\n",
    "X_train = full_pipe.fit_transform(X_train)\n",
    "X_test = full_pipe.transform(X_test)\n",
    "\n",
    "#joblib.dump(X_train, \"X_train.pkl\")\n",
    "#joblib.dump(X_test, \"X_test.pkl\")\n",
    "\n",
    "#joblib.dump(y_train, \"y_train.pkl\")\n",
    "#joblib.dump(y_test, \"y_test.pkl\")\n",
    "\n",
    "print(\"SHAPE OF X_train\", X_train.shape)\n",
    "print(\"type(X_train) = \", type(X_train))\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "# # # # FLAVIEN\n",
    "###############\n",
    "# rajouter les engineered features aux sets prcq lÃ  j'entraines mes models uniquements sur train[text]\n",
    "\n",
    "# # # # SELECT AND TRAIN MODELS # # # #\n",
    "train_mae_scores = []\n",
    "test_mae_scores = []\n",
    "\n",
    "# Linear Regressor\n",
    "print(\"Linear Regressor\")\n",
    "\n",
    "# Lasso Regressor\n",
    "print(\"Lasso Regressor\")\n",
    "\n",
    "# Ridge Regressor\n",
    "print(\"Ridge Regressor\")\n",
    "\n",
    "# Elastic Net Regressor\n",
    "print(\"Elastic Net Regressor\")\n",
    "\n",
    "# GradientBoostingRegressor\n",
    "print(\"GradientBoostingRegressor\")\n",
    "gb_reg = GradientBoostingRegressor(criterion='mse')\n",
    "gb_reg.fit(X_train, y_train)\n",
    "y_pred_gb_reg_train = gb_reg.predict(X_train)\n",
    "y_pred_gb_reg_test = gb_reg.predict(X_test)\n",
    "gb_reg_train_mae = mean_absolute_error(y_true=y_train, y_pred=y_pred_gb_reg_train)\n",
    "gb_reg_test_mae = mean_absolute_error(y_true=y_test, y_pred=y_pred_gb_reg_test)\n",
    "print(\"Prediction error:\", gb_reg_train_mae, gb_reg_test_mae)\n",
    "train_mae_scores.append(gb_reg_train_mae)\n",
    "test_mae_scores.append(gb_reg_test_mae)\n",
    "\n",
    "joblib.dump(gb_reg, \"gb_reg.pkl\")\n",
    "\n",
    "\n",
    "# Decision Tree Regressor\n",
    "print(\"DecisionTreeRegressor\")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(criterion='mse')\n",
    "tree_reg.fit(X_train, y_train)\n",
    "pred_tree_reg_train = tree_reg.predict(X_train)\n",
    "pred_tree_reg_test = tree_reg.predict(X_test)\n",
    "tree_reg_train_mae = mean_absolute_error(y_true=y_train, y_pred=pred_tree_reg_train)\n",
    "tree_reg_test_mae = mean_absolute_error(y_true=y_test, y_pred=pred_tree_reg_test)\n",
    "print(\"Dec Tree prediction error:\", tree_reg_train_mae, tree_reg_test_mae)\n",
    "train_mae_scores.append(tree_reg_train_mae)\n",
    "test_mae_scores.append(tree_reg_test_mae)\n",
    "\n",
    "joblib.dump(tree_reg, \"tree_reg.pkl\")\n",
    "\n",
    "\n",
    "# Random Forest Regressor\n",
    "print(\"RandomForestRegressor\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rdf_reg = RandomForestRegressor(n_estimators=500,  criterion='mse', max_depth=5, max_leaf_nodes=16, n_jobs=-1)\n",
    "rdf_reg.fit(X_train, y_train)\n",
    "pred_rdf_reg_train = rdf_reg.predict(X_train)\n",
    "pred_rdf_reg_test = rdf_reg.predict(X_test)\n",
    "rdf_reg_train_mae = mean_absolute_error(y_true=y_train, y_pred=pred_rdf_reg_train)\n",
    "rdf_reg_test_mae = mean_absolute_error(y_true=y_test, y_pred=pred_rdf_reg_test)\n",
    "print(\"Rand For prediction error:\", rdf_reg_train_mae, rdf_reg_test_mae)\n",
    "train_mae_scores.append(rdf_reg_train_mae)\n",
    "test_mae_scores.append(rdf_reg_test_mae)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"LogisticRegression\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "pred_log_reg_train = log_reg.predict(X_train)\n",
    "pred_log_reg_test = log_reg.predict(X_test)\n",
    "log_reg_train_mae = mean_absolute_error(y_true=y_train, y_pred=pred_log_reg_train)\n",
    "log_reg_test_mae = mean_absolute_error(y_true=y_test, y_pred=pred_log_reg_test)\n",
    "print(\"Log Reg prediction error:\", log_reg_train_mae, log_reg_test_mae)\n",
    "train_mae_scores.append(log_reg_train_mae)\n",
    "test_mae_scores.append(log_reg_test_mae)\n",
    "\n",
    "# Linear SVC\n",
    "print(\"LinearSVC\")\n",
    "from sklearn.svm import LinearSVC\n",
    "linSVC = LinearSVC()\n",
    "linSVC.fit(X_train, y_train)\n",
    "pred_linSVC_train = linSVC.predict(X_train)\n",
    "pred_linSVC_test = linSVC.predict(X_test)\n",
    "linSVC_train_mae = mean_absolute_error(y_true=y_train, y_pred=pred_linSVC_train)\n",
    "linSVC_test_mae = mean_absolute_error(y_true=y_test, y_pred=pred_linSVC_test)\n",
    "print(\"Lin SVC prediction error:\", linSVC_train_mae, linSVC_test_mae)\n",
    "train_mae_scores.append(linSVC_train_mae)\n",
    "test_mae_scores.append(linSVC_test_mae)\n",
    "\n",
    "\n",
    "# # # # RANKING OF MODELS # # # #\n",
    "\n",
    "estimators = ['GradientBoostingRegressor',\n",
    "              'DecisionTreeRegressor',\n",
    "              'RandomForestRegressor',\n",
    "              'LogisticRegression',\n",
    "              'LinearSVC']\n",
    "\n",
    "bar_width = 0.10\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(len(estimators))\n",
    "training_scores1 = plt.barh(index, train_mae_scores, bar_width, color='darkred', alpha=0.6, label='Training Scores')\n",
    "test_scores1 = plt.barh(index+bar_width, test_mae_scores, bar_width, color='darkgreen', alpha=0.6, label='Test Scores')\n",
    "ax.set_title(\"Ranking of models by MAE scores\", fontsize=15, weight='bold')\n",
    "ax.set_xlabel('MAE')\n",
    "ax.set_ylabel('Estimators')\n",
    "ax.set_yticks(index+bar_width/2)\n",
    "ax.set_yticklabels(estimators)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
